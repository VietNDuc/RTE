{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"e34bcb1a-7fc5-4e4a-b322-5bc2a4f7d667","_cell_guid":"48026241-ff1e-4c5d-9172-6f661a434563","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/stanford-natural-language-inference-corpus/README.txt\n/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_test.csv\n/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_train.csv\n/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_dev.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertModel\nimport transformers\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom ignite.engine import Engine, Events\nfrom ignite.metrics import Accuracy, Loss\nfrom ignite.contrib.handlers.param_scheduler import create_lr_scheduler_with_warmup, LRScheduler\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom ignite.handlers import EarlyStopping, Checkpoint, DiskSaver, global_step_from_engine\nfrom ignite.engine.events import EventEnum\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\ntorch.backends.cudnn.benchmark = True","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\nfor param in model.base_model.parameters():\n    param.requires_grad = False","metadata":{"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ffe7d00172e4bf684157300ef539bb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d0c63aa55794705a6f0e4c12ce199ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e389d287340645ccaca33368ae724d63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a37d4040ee4e218635babf9eaac295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d4c3ef7a6d44aca05b6b3c821db29e"}},"metadata":{}}]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_test.csv', \n                 usecols=[\"gold_label\", \"sentence1\", \"sentence2\"])\ndf = df.loc[df[\"gold_label\"] != \"-\"]","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Dataset_from_encoding(Dataset):\n    \"\"\"\n    \"\"\"\n    def __init__(self, p_encodings, h_encodings, labels):\n        self.p_encodings = p_encodings\n        self.h_encodings = h_encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {}\n        item[\"p\"] = {key: val[idx].clone().detach() for key, val in self.p_encodings.items()}\n        item[\"h\"] = {key: val[idx].clone().detach() for key, val in self.h_encodings.items()}\n        item[\"labels\"] = torch.tensor(self._get_label(self.labels[idx]))\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n    \n    def _get_label(self, x):\n        label = {'contradiction': 0,\n                 'neutral': 1,\n                 'entailment': 2,}\n\n        return label[x]","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_train_test(df, test_size=0.2):\n    \"\"\"\n    \"\"\"\n    train, test = train_test_split(df, test_size=test_size, shuffle=True)\n    train_p_encodings = tokenizer(train.sentence1.tolist(), \n                                return_tensors=\"pt\",\n                                max_length=128,\n                                truncation=True,\n                                padding=True).to(device)\n    train_h_encodings = tokenizer(train.sentence2.tolist(), \n                                return_tensors=\"pt\",\n                                max_length=128,\n                                truncation=True,\n                                padding=True).to(device)\n    test_p_encodings = tokenizer(test.sentence1.tolist(), \n                                return_tensors=\"pt\",\n                                max_length=128,\n                                truncation=True,\n                                padding=True).to(device)\n    test_h_encodings = tokenizer(test.sentence2.tolist(), \n                               return_tensors=\"pt\",\n                               max_length=128,\n                               truncation=True,\n                               padding=True).to(device)\n\n    train_ds = Dataset_from_encoding(train_p_encodings, train_h_encodings, train[\"gold_label\"].tolist())\n    test_ds = Dataset_from_encoding(test_p_encodings, test_h_encodings, test[\"gold_label\"].tolist())\n\n    return train_ds, test_ds","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_ds, test_ds = get_train_test(df)","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dl = DataLoader(train_ds, 64, shuffle=True)\ntest_dl = DataLoader(test_ds, 64, shuffle=False)","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class mGRU(nn.Module):\n    \"\"\"matchLSTM implementation but using GRU instead of LSTM\n    \"\"\"\n    def __init__ (self, options):\n        super(mGRU, self).__init__()\n        self.options = options\n        self.n_embed = 768\n        self.n_dim = 300\n        self.n_out = 3\n        if 'USE_PRETRAINED' in self.options.keys():\n            embed_matrix = self.l_en.get_embedding_matrix()\n            if embed_matrix is not None:\n                print(f\"Embedding matrix size {embed_matrix.shape}\")\n                self.embedding.weight = nn.Parameter(torch.Tensor(embed_matrix))\n\n        self.premise_gru = nn.GRU(self.n_embed, self.n_dim, bidirectional=False).to(device)\n        self.hypothesis_gru = nn.GRU(self.n_embed, self.n_dim, bidirectional=False).to(device)\n        self.out = nn.Linear(self.n_dim, self.n_out).to(device)\n\n        # Attention Parameters\n        if self.options[\"CUDA\"]:\n            self.W_s = nn.Parameter(torch.randn(self.n_dim, self.n_dim).cuda())  # n_dim x n_dim\n            self.register_parameter('W_s', self.W_s)\n            self.W_t = nn.Parameter(torch.randn(self.n_dim, self.n_dim).cuda())  # n_dim x n_dim\n            self.register_parameter('W_t', self.W_t)\n            self.w_e = nn.Parameter(torch.randn(self.n_dim, 1).cuda()) # n_dim x 1\n            self.register_parameter('w_e', self.w_e)\n            self.W_m = nn.Parameter(torch.randn(self.n_dim, self.n_dim).cuda())  # n_dim x n_dim\n            self.register_parameter('W_m', self.W_m)\n        else:\n            self.W_s = nn.Parameter(torch.randn(self.n_dim, self.n_dim))\n            self.register_parameter('W_s', self.W_s)\n            self.W_t = nn.Parameter(torch.randn(self.n_dim, self.n_dim))\n            self.register_parameter('W_t', self.W_t)\n            self.w_e = nn.Parameter(torch.randn(self.n_dim, 1))\n            self.register_parameter('w_e', self.w_e)\n            self.W_m = nn.Parameter(torch.randn(self.n_dim, self.n_dim))\n            self.register_parameter('W_m', self.W_m)\n\n        # Match GRU parameters.\n        self.m_gru = nn.GRU(self.n_dim + self.n_dim, self.n_dim, bidirectional=False).to(device)\n\n    def _init_hidden(self, batch_size):\n        \"\"\"Init hidden matrix for GRU\"\"\"\n        hidden_p = Variable(torch.zeros(1, batch_size, self.n_dim))\n        hidden_h = Variable(torch.zeros(1, batch_size, self.n_dim))\n        return hidden_p, hidden_h\n\n    def _attn_gru_init_hidden(self, batch_size):\n        \"\"\"Init for GRU attention\"\"\"\n        r_0 = Variable(torch.zeros(batch_size, self.n_dim))\n        return r_0\n\n    def mask_mult(self, o_t, o_tm1, mask_t):\n        \"\"\"\"\"\"\n        return (o_t.to(device) * mask_t.to(device)) + (o_tm1.to(device) * (torch.logical_not(mask_t.to(device))))\n    \n    def _gru_forward(self, gru, encoded_sent, mask_sent, h_0):\n        \"\"\"Stateful GRU for premise/hypothesis\n\n        Parameters:\n        ----\n        gru: GRU cell\n        encoded_sent: embedded matrix of premise/hypothesis sentence\n        mask_sent: mask vector for embedded matrix\n        h_0: init hidden vector for GRU cell\n\n        Returns:\n        ----\n        o_s: output of last timestep in each batch from GRU cell. A matrix has shape (T x batch x n_dim)\n        h_t: last hidden state vector (1 x batch x n_dim)\n        \"\"\"\n        len_seq = encoded_sent.size(0)\n        batch_size = encoded_sent.size(1)\n        o_s = Variable(torch.zeros(len_seq, batch_size, self.n_dim))\n        h_tm1 = h_0.squeeze(0)\n        o_tm1 = None\n\n        for ix, (x_t, mask_t) in enumerate(zip(encoded_sent, mask_sent)):\n            '''\n            x_t : batch x n_embed; \n            mask_t : batch,\n            '''\n            o_t, h_t = gru(x_t.unsqueeze(0).to(device), \n                           h_tm1.unsqueeze(0).to(device))  # 1 x batch x n_dim\n            mask_t = mask_t.unsqueeze(1)  # batch x 1\n            h_t = self.mask_mult(h_t[0], h_tm1, mask_t)\n\n            if o_tm1 is not None:\n                o_t = self.mask_mult(o_t[0], o_tm1, mask_t)\n            o_tm1 = o_t[0] if o_tm1 is None else o_t\n            h_tm1 = h_t\n            o_s[ix] = o_t\n\n        return o_s, h_t.unsqueeze(0)\n\n    def _attention_forward(self, H_s, mask_H_s, h_t, h_m_tm1=None):\n        '''Word-by-word attention.\n\n        Computes the Attention Weights over H_s using h_t (and h_m_tm1 if given)\n        Returns an attention weighted representation of H_s, and the alphas.\n\n        Parameters:\n        ----\n            H_s (T x batch x n_dim): output of all batchs come from GRU cell\n            mask_Y (T x batch): mask matrix\n            h_t (batch x n_dim): hidden matrix for t-th word in hypothesis (batch)\n            h_m_tm1 (batch x n_dim): previous h_m\n\n        Returns:\n        ----\n            h_m (batch x n_dim)\n            alpha (batch x T): attention weight\n        '''\n        H_s = H_s.transpose(1, 0).cuda()  # batch x T x n_dim\n        mask_H_s = mask_H_s.transpose(1, 0)  # batch x T\n\n        Whs = torch.bmm(H_s, self.W_s.unsqueeze(0).expand(H_s.size(0), *self.W_s.size()))  # batch x T x n_dim\n        Wht = torch.mm(h_t.cuda(), self.W_t)  # batch x n_dim\n        if h_m_tm1 is not None:\n            W_r_tm1 = torch.mm(h_m_tm1.cuda(), self.W_m)  # (batch, n_dim)\n            Whs += W_r_tm1.unsqueeze(1)\n        M = torch.tanh(Whs + Wht.unsqueeze(1).expand(Wht.size(0), H_s.size(1), Wht.size(1)))  # batch x T x n_dim\n        alpha = torch.bmm(M, self.w_e.unsqueeze(0).expand(H_s.size(0), *self.w_e.size())).squeeze(-1)  # batch x T\n        alpha = alpha + (-1000.0 * (torch.logical_not(mask_H_s)))  # To ensure probability mass doesn't fall on non tokens\n        alpha = F.softmax(alpha)\n        return torch.bmm(alpha.unsqueeze(1), H_s).squeeze(1), alpha\n\n    def _attn_gru_forward(self, o_h, mask_h, r_0, o_p, mask_p):\n        '''Use match-GRU to modeling the matching between the premise and the hypothesis.\n\n        Parameters:\n        ----\n        o_h : T x batch x n_dim : The hypothesis\n        mask_h : T x batch\n        r_0 : batch x n_dim :\n        o_p : T x batch x n_dim : The premise. Will attend on it at every step\n        mask_p : T x batch : the mask for the premise\n\n        Returns:\n        ----\n            r : batch x n_dim : the last state of the rnn\n            alpha_vec : T x batch x T the attn vec at every step\n        '''\n        seq_len_h = o_h.size(0)\n        batch_size = o_h.size(1)\n        seq_len_p = o_p.size(0)\n        alpha_vec = Variable(torch.zeros(seq_len_h, batch_size, seq_len_p))\n        r_tm1 = r_0\n        for ix, (h_t, mask_t) in enumerate(zip(o_h, mask_h)):\n            '''\n                h_t : batch x n_dim\n                mask_t : batch,\n            '''\n            a_t, alpha = self._attention_forward(o_p, mask_p, h_t, r_tm1)   # a_t : batch x n_dim\n                                                                            # alpha : batch x T                                                                         \n            alpha_vec[ix] = alpha\n            m_t = torch.cat([a_t, h_t.cuda()], dim=-1)\n            r_t, _ = self.m_gru(m_t.unsqueeze(0).to(device), \n                                r_tm1.unsqueeze(0).to(device))\n\n            mask_t = mask_t.unsqueeze(1)  # batch x 1\n            r_t = self.mask_mult(r_t[0], r_tm1, mask_t)\n            r_tm1 = r_t\n\n        return r_t, alpha_vec\n\n    def forward(self, encoded_p, encoded_h, training):\n        \"\"\"\n        encoded_p (seq_len, batch, n_dim): encoding matrix premise\n        encoded_h (seq_len, batch, n_dim): encoding matrix of hypothesis\n        \"\"\"\n        batch_size = encoded_p.size(1)\n\n        mask_p = torch.any(torch.ne(encoded_p, 0), axis=2)\n        mask_h = torch.any(torch.ne(encoded_h, 0), axis=2)\n\n        encoded_p = F.dropout(encoded_p, p=self.options[\"DROPOUT\"], training=training)\n        encoded_h = F.dropout(encoded_h, p=self.options[\"DROPOUT\"], training=training)\n\n        # RNN\n        h_p_0, h_n_0 = self._init_hidden(batch_size)  # 1 x batch x n_dim\n        o_p, _ = self._gru_forward(self.premise_gru, encoded_p, mask_p, h_p_0)\n        o_h, _ = self._gru_forward(self.hypothesis_gru, encoded_h, mask_h, h_n_0)\n        # Attention\n        r_0 = self._attn_gru_init_hidden(batch_size)\n        h_star, _ = self._attn_gru_forward(o_h, mask_h, r_0, o_p, mask_p)\n        # Output layer\n        h_star = self.out(h_star.cuda())\n        if self.options[\"LAST_NON_LINEAR\"]:\n            h_star = F.relu(h_star)\n        pred = F.log_softmax(h_star)\n\n        return pred","metadata":{"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"options = {\"HIDDEN_DIM\": 300, \"CLASSES_2_IX\": 3, \"DROPOUT\":0.2, \"LAST_NON_LINEAR\": True, \"CUDA\": True}\nmgru = mGRU(options)","metadata":{"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss().cuda()\nparam_optimizer = list(mgru.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n             ]\noptim_params = {'lr': 2e-2, 'eps': 1e-6,} #'correct_bias': False}\noptim = transformers.AdamW(optimizer_grouped_parameters, **optim_params)","metadata":{"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"scheduler = transformers.get_linear_schedule_with_warmup(optim, 10000, len(train_dl)*10)","metadata":{"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def train_step(engine, batch):\n    mgru.train()\n    optim.zero_grad()\n    y = batch[\"labels\"].to(device)\n    p_encode = model(**batch[\"p\"])[\"last_hidden_state\"].permute(1, 0, 2)\n    h_encode = model(**batch[\"h\"])[\"last_hidden_state\"].permute(1, 0, 2)\n    y_pred = mgru(p_encode, h_encode, training=True)\n    loss = criterion(y_pred, y)\n    loss.backward()\n    # engine.fire_event(BackpropEvents.BACKWARD_COMPLETED)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n    optim.step()\n    # scheduler.step()\n    return loss.item()\n\n\ndef validation_step(engine, batch):\n    model.eval()\n    y = batch[\"labels\"].to(device)\n    p_encode = model(**batch[\"p\"])[\"last_hidden_state\"].permute(1, 0, 2)\n    h_encode = model(**batch[\"h\"])[\"last_hidden_state\"].permute(1, 0, 2)\n    y_pred = mgru(p_encode, h_encode, training=False)\n    return y_pred, y\n    \n\ndef score_function(engine):\n    return engine.state.metrics['accuracy']","metadata":{"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"log_interval = 10\npbar = tqdm(initial=0, leave=False, total=len(train_dl), desc=f\"ITERATION - loss: {0:.2f}\")\n\ntrainer = Engine(train_step)\n# trainer.register_events(*BackpropEvents)\n\nval_metrics = {\n    \"accuracy\": Accuracy(),\n    \"loss\": Loss(criterion)\n}\nevaluator = Engine(validation_step)\nfor name, metric in val_metrics.items():\n    metric.attach(evaluator, name)\n\nhandler = EarlyStopping(patience=5, score_function=score_function, trainer=trainer)\nevaluator.add_event_handler(Events.COMPLETED, handler)\n\n@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\ndef log_training_loss(engine):\n    # print(\"Epoch[{}] Loss: {:.2f}\".format(trainer.state.epoch, trainer.state.output))\n    pbar.desc = f\"ITERATION - loss: {engine.state.output:.2f}\"\n    pbar.update(log_interval)\n\n@trainer.on(Events.EPOCH_COMPLETED)\ndef log_validation_results(engine):\n    evaluator.run(test_dl)\n    metrics = evaluator.state.metrics\n    tqdm.write(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n          .format(trainer.state.epoch, metrics[\"accuracy\"], metrics[\"loss\"]))\n\n    pbar.n = pbar.last_print_n = 0\n\n\n# @evaluator.on(Events.EPOCH_COMPLETED)\n# def reduct_step(engine):\n#     scheduler.step()\n\n\n@trainer.on(Events.EPOCH_COMPLETED | Events.COMPLETED)\ndef log_time(engine):\n    tqdm.write(f\"{trainer.last_event_name.name} took {trainer.state.times[trainer.last_event_name.name]} seconds\")\n\ntrainer.run(train_dl, 10)\npbar.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}