{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "48026241-ff1e-4c5d-9172-6f661a434563",
    "_uuid": "e34bcb1a-7fc5-4e4a-b322-5bc2a4f7d667",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.handlers.param_scheduler import create_lr_scheduler_with_warmup, LRScheduler\n",
    "from ignite.handlers import EarlyStopping, Checkpoint, DiskSaver, global_step_from_engine\n",
    "from ignite.engine.events import EventEnum\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.mgru import mGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffe7d00172e4bf684157300ef539bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0c63aa55794705a6f0e4c12ce199ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e389d287340645ccaca33368ae724d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a37d4040ee4e218635babf9eaac295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d4c3ef7a6d44aca05b6b3c821db29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_test.csv', \n",
    "                 usecols=[\"gold_label\", \"sentence1\", \"sentence2\"])\n",
    "df = df.loc[df[\"gold_label\"] != \"-\"]\n",
    "df.loc[:, [\"sentence1\", \"sentence2\"]] = df.loc[:, [\"sentence1\", \"sentence2\"]].apply(lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_from_encoding(Dataset):\n",
    "    \"\"\"Create dataset from encoding matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, p_encodings, h_encodings, labels):\n",
    "        self.p_encodings = p_encodings\n",
    "        self.h_encodings = h_encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        item[\"p\"] = {key: val[idx].clone().detach() for key, val in self.p_encodings.items()}\n",
    "        item[\"h\"] = {key: val[idx].clone().detach() for key, val in self.h_encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self._get_label(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_label(self, x):\n",
    "        label = {'contradiction': 0,\n",
    "                 'neutral': 1,\n",
    "                 'entailment': 2,}\n",
    "\n",
    "        return label[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(df, test_size=0.2):\n",
    "    \"\"\"Train/test split & create Dataset\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(df, test_size=test_size, shuffle=True)\n",
    "    train_p_encodings = tokenizer(train.sentence1.tolist(), \n",
    "                                return_tensors=\"pt\",\n",
    "                                max_length=64,\n",
    "                                truncation=True,\n",
    "                                padding=True).to(device)\n",
    "    train_h_encodings = tokenizer(train.sentence2.tolist(), \n",
    "                                return_tensors=\"pt\",\n",
    "                                max_length=64,\n",
    "                                truncation=True,\n",
    "                                padding=True).to(device)\n",
    "    test_p_encodings = tokenizer(test.sentence1.tolist(), \n",
    "                                return_tensors=\"pt\",\n",
    "                                max_length=64,\n",
    "                                truncation=True,\n",
    "                                padding=True).to(device)\n",
    "    test_h_encodings = tokenizer(test.sentence2.tolist(), \n",
    "                               return_tensors=\"pt\",\n",
    "                               max_length=64,\n",
    "                               truncation=True,\n",
    "                               padding=True).to(device)\n",
    "\n",
    "    train_ds = Dataset_from_encoding(train_p_encodings, train_h_encodings, train[\"gold_label\"].tolist())\n",
    "    test_ds = Dataset_from_encoding(test_p_encodings, test_h_encodings, test[\"gold_label\"].tolist())\n",
    "\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, 32, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"EMBED_DIM\": 300, \"HIDDEN_DIM\": 150, \"CLASSES\": 3, \"DROPOUT\": 0.2, \"LAST_NON_LINEAR\": True, \"CUDA\": use_cuda}\n",
    "mgru = mGRU(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "optim = transformers.AdamW(mgru.parameters(), lr=1e-4, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = transformers.get_linear_schedule_with_warmup(optim, 10000, len(train_dl)*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(engine, batch):\n",
    "    mgru.train()\n",
    "    optim.zero_grad()\n",
    "    y = batch[\"labels\"].to(device)\n",
    "    p_encode = model(**batch[\"p\"])[\"last_hidden_state\"].permute(1, 0, 2)\n",
    "    h_encode = model(**batch[\"h\"])[\"last_hidden_state\"].permute(1, 0, 2)\n",
    "    y_pred = mgru(p_encode, h_encode, training=True)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    # engine.fire_event(BackpropEvents.BACKWARD_COMPLETED)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "    optim.step()\n",
    "    # scheduler.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    y = batch[\"labels\"].to(device)\n",
    "    p_encode = model(**batch[\"p\"])[\"last_hidden_state\"].permute(1, 0, 2)\n",
    "    h_encode = model(**batch[\"h\"])[\"last_hidden_state\"].permute(1, 0, 2)\n",
    "    y_pred = mgru(p_encode, h_encode, training=False)\n",
    "    return y_pred, y\n",
    "    \n",
    "\n",
    "def score_function(engine):\n",
    "    return engine.state.metrics['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "pbar = tqdm(initial=0, leave=False, total=len(train_dl), desc=f\"ITERATION - loss: {0:.2f}\")\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"loss\": Loss(criterion)\n",
    "}\n",
    "evaluator = Engine(validation_step)\n",
    "for name, metric in val_metrics.items():\n",
    "    metric.attach(evaluator, name)\n",
    "\n",
    "handler = EarlyStopping(patience=5, score_function=score_function, trainer=trainer)\n",
    "evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_training_loss(engine):\n",
    "    # print(\"Epoch[{}] Loss: {:.2f}\".format(trainer.state.epoch, trainer.state.output))\n",
    "    pbar.desc = f\"ITERATION - loss: {engine.state.output:.2f}\"\n",
    "    pbar.update(log_interval)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(test_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    tqdm.write(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "          .format(trainer.state.epoch, metrics[\"accuracy\"], metrics[\"loss\"]))\n",
    "\n",
    "    pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "# @evaluator.on(Events.EPOCH_COMPLETED)\n",
    "# def reduct_step(engine):\n",
    "#     scheduler.step()\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED | Events.COMPLETED)\n",
    "def log_time(engine):\n",
    "    tqdm.write(f\"{trainer.last_event_name.name} took {trainer.state.times[trainer.last_event_name.name]} seconds\")\n",
    "\n",
    "trainer.run(train_dl, EPOCHS)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow():\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in mgru.named_parameters():\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.mean())\n",
    "            max_grads.append(p.grad.max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02)\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.show()\n",
    "\n",
    "plot_grad_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
